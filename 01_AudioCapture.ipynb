{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e9c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import queue\n",
    "import threading\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import base64\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_api_key=os.getenv(\"AZURE_OPENAI_GPT4O_API_KEY\")\n",
    "azure_endpoint=os.getenv(\"AZURE_OPENAI_GPT4O_ENDPOINT\")\n",
    "azure_deployment=os.getenv(\"AZURE_OPENAI_GPT4O_DEPLOYMENT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1954fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Audio Queue\n",
    "AUDIO_QUEUE = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9576fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_capture(stop_event):\n",
    "    \"\"\"Capture audio from microphone and add to queue\"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=pyaudio.paInt16,  # 16-bit PCM (pcm16)\n",
    "        channels=1,              # Mono audio\n",
    "        rate=24000,              # 24kHz as recommended by OpenAI\n",
    "        input=True,\n",
    "        frames_per_buffer=1024,  # Number of frames per buffer\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ™ï¸ Recording started...\")\n",
    "\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            data = stream.read(num_frames=1024, exception_on_overflow=False)\n",
    "            if stop_event.is_set():\n",
    "                break\n",
    "            AUDIO_QUEUE.put(data)\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        print(\"ğŸ™ï¸ Recording stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e275662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wss://mmx-aoai.openai.azure.com//openai/realtime?api-version=2025-04-01-preview&deployment=gpt-4o-mini-transcribe\n"
     ]
    }
   ],
   "source": [
    "headers = {\"api-key\": azure_api_key}\n",
    "ws_url = f\"wss://{azure_endpoint}/openai/realtime?intent=transcription&deployment={azure_deployment}&api-version=2024-10-01-preview\"\n",
    "ws_url = f\"{os.environ.get('AZURE_OPENAI_STT_TTS_ENDPOINT').replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&deployment=gpt-4o-mini-transcribe\"\n",
    "print(ws_url)\n",
    "config = {\n",
    "    \"type\": \"transcription_session.update\",\n",
    "    \"session\": {\n",
    "        \"input_audio_format\": \"pcm16\",\n",
    "        \"input_audio_transcription\": {\"model\": \"gpt-4o-mini-transcribe\"},\n",
    "        \"turn_detection\": {\n",
    "            \"type\": \"server_vad\",\n",
    "            \"threshold\": 0.5,\n",
    "            \"prefix_padding_ms\": 300,\n",
    "            \"silence_duration_ms\": 500,\n",
    "        },\n",
    "        \"input_audio_noise_reduction\": {\n",
    "            \"type\": \"near_field\"\n",
    "        } # Use no noise reduction for now\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aadf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_audio(websocket, stop_event):\n",
    "    \"\"\"Send audio data to the WebSocket server\"\"\"\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            if not AUDIO_QUEUE.empty():\n",
    "                audio_data = AUDIO_QUEUE.get()\n",
    "                \n",
    "                # Encode audio data as base64\n",
    "                encoded_data = base64.b64encode(audio_data).decode(\"utf-8\")\n",
    "\n",
    "                # Create audio buffer message\n",
    "                message = {\n",
    "                    \"type\": \"input_audio_buffer.append\",\n",
    "                    \"audio\": encoded_data,\n",
    "                }\n",
    "                await websocket.send(json.dumps(message))\n",
    "            await asyncio.sleep(0.01)  # Small delay to prevent busy waiting\n",
    "    except websockets.ConnectionClosed:\n",
    "        print(\"send_audio: WebSocket connection closed\")\n",
    "    except asyncio.CancelledError as e:\n",
    "        print(f\"send_audio: Task cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c0dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def receive_messages(websocket, stop_event):\n",
    "    \"\"\"Receive messages from the WebSocket server\"\"\"\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                message = await websocket.recv()\n",
    "                data = json.loads(message)\n",
    "                if \"type\" in data and data[\"type\"] == \"input_audio_buffer.speech_started\":\n",
    "                    print(\"ğŸ¤ Speech Detected\")\n",
    "                elif \"type\" in data and data[\"type\"] == \"input_audio_buffer.speech_stopped\":\n",
    "                    print(\"ğŸ”‡Speech Stopped\")\n",
    "                elif \"type\" in data and data[\"type\"] == \"conversation.item.input_audio_transcription.completed\":\n",
    "                    # Transcription utterance completed\n",
    "                    transcript_raw = data.get(\"transcript\", \"\")\n",
    "                    transcript_json = json.loads(transcript_raw)\n",
    "                    transcript = transcript_json.get(\"text\", \"\")\n",
    "                    print(f'\\nğŸ“ Azure Completed Transcript: \"{transcript}\"', flush=True)\n",
    "                elif \"type\" in data and data[\"type\"] == \"response.text.delta\":\n",
    "                    delta_text = data.get(\"delta\", \"\")\n",
    "                    if delta_text:\n",
    "                        print(f\"ğŸ“¥: {delta_text}\")\n",
    "                elif \"type\" in data and data[\"type\"]  == \"response.text.done\":\n",
    "                    final_text = data.get(\"text\", \"\")\n",
    "                    if final_text:\n",
    "                        print(\"ğŸ“¨:\", final_text)\n",
    "                else:\n",
    "                    pass\n",
    "                    # Implement other message types as needed\n",
    "            except websockets.ConnectionClosed:\n",
    "                print(\"Connection closed\")\n",
    "                break\n",
    "    except asyncio.CancelledError as e:\n",
    "        print(f\"receive_messages: Task cancelled\")\n",
    "    except websockets.ConnectionClosed as e:\n",
    "        print(f\"receive_messages: WebSocket connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ccde5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Recording started...\n",
      "ğŸ”— WebSocket connection established\n",
      "Speak into the microphone...\n",
      "ğŸ¤ Speech Detected\n",
      "ğŸ”‡Speech Stopped\n",
      "ğŸ“¥: Hi\n",
      "ğŸ“¥:  there\n",
      "ğŸ“¥: !\n",
      "ğŸ“¨: Hi there!\n",
      "ğŸ¤ Speech Detected\n",
      "ğŸ”‡Speech Stopped\n",
      "ğŸ“¥: This\n",
      "ğŸ“¥:  is\n",
      "ğŸ“¥:  a\n",
      "ğŸ“¥:  demo\n",
      "ğŸ“¥: .\n",
      "ğŸ“¨: This is a demo.\n",
      "ğŸ›‘ Stopping...\n",
      "ğŸ™ï¸ Recording stopped\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Clear audio queue\n",
    "while not AUDIO_QUEUE.empty():\n",
    "    AUDIO_QUEUE.get()\n",
    "    \n",
    "    \n",
    "stop_event = threading.Event()\n",
    "# This will run the audio capture in a separate thread\n",
    "audio_thread = threading.Thread(target=audio_capture, args=(stop_event,))\n",
    "audio_thread.daemon = True\n",
    "audio_thread.start()\n",
    "    \n",
    "async with websockets.connect(\n",
    "    ws_url, additional_headers=headers\n",
    ") as websocket:\n",
    "    try:\n",
    "        print(\"ğŸ”— WebSocket connection established\")\n",
    "        print(\"Speak into the microphone...\")\n",
    "\n",
    "        # Setup the transcription session\n",
    "        await websocket.send(json.dumps(config))\n",
    "        \n",
    "        # Create tasks for sending audio and receiving messages\n",
    "        send_task = asyncio.create_task(send_audio(websocket, stop_event))\n",
    "        receive_task = asyncio.create_task(receive_messages(websocket, stop_event))\n",
    "        \n",
    "        # Wait until any one the task finishes\n",
    "        try:\n",
    "            done, pending = await asyncio.wait(\n",
    "                {send_task, receive_task}, return_when=asyncio.FIRST_COMPLETED\n",
    "            )\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"ğŸ›‘ Stopping...\")\n",
    "        \n",
    "    finally:\n",
    "        if not stop_event.is_set():\n",
    "            stop_event.set()\n",
    "        if audio_thread.is_alive():\n",
    "            audio_thread.join(timeout=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba204267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
